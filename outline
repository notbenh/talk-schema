overnormalization
yes this is going to be the developers introduction to normalization, I went to far and found a lot of good in crossing the line of sanity.
but I'm getting ahead of my self, lets back up and set up a foundation
like most places over time, your schemas become a mess.
we had many generations of many diffrent attempts at cleaning things up, but there was still cruft.
Most of the cruft was due to the tables being so large and live so you can't alter them.
example: author->contributor
to make matters worse books are odd in that the unique id is a combo-key for title+pub+binding+edition
resulted in a layout where entire schemas were being filled by completely diffrent processes
example: title cascade
This only highlighted a huge hole where we were not capturing any info for missing books and it became a problem for OBB.
Thus the problem needed to be solved
Options on the table: 
- clean up and continue the existing 'normal' mindset where you have some properly grouped tables for data
-- pro: it's standard, every one would understand it
-- con: it's just a temp patch as this is the same way that every prevous attempt was solved
-- con: still have that alter table problem
-- con: source/stamp feature not easy to solve
- go balls to the wall and blow everything apart
-- pro: flexable and almost future proof
-- pro: lends it's self to source/stamp method
-- con: untested, would it work? 
As you can guess, because I'm talking here, we didn't go with the 'normal method'

So lets look at an example of the old fat row idea that we've moved away from.

http://www.powells.com/biblio/9780596100926


ISBN: 9780596100926
Title: Perl Testing: A Developer's Notebook
Author: Ian Langworth and Chromatic
Author: Langworth, Ian
Author: Chromatic
Publisher: O'Reilly Media
Subject: Programming Languages - CGI, Javascript, Perl, VBScript
Subject: Perl (computer program language)
Subject: Programming Languages - Perl
Subject: Programming - Quality Assurance & Testing
Subject: Software Development & Engineering - Quality
Copyright: 2005
Publication Date: July 2005
Binding: Hardcover
Language: English
Illustrations: Y
Pages: 180
Dimensions: 9.22x7.10x.55 in. .78 lbs.
So lets dive in to the quickie normalization discussion:

1NF: no repeating groups

Author & Subject are many to one relationships. So lets pull them to there own tables.

Author: Ian Langworth and Chromatic
Author: Langworth, Ian
Author: Chromatic

Subject: Programming Languages - CGI, Javascript, Perl, VBScript
Subject: Perl (computer program language)
Subject: Programming Languages - Perl
Subject: Programming - Quality Assurance & Testing
Subject: Software Development & Engineering - Quality

ISBN: 9780596100926
Subtitle: A Developer's Notebook
Publisher: O'Reilly Media
Copyright: 2005
Publication Date: July 2005
Binding: Hardcover
Language: English
Illustrations: Y
Pages: 180
Dimensions: 9.22x7.10x.55 in. .78 lbs.

2NF:
1971)[13] No non-prime attribute in the table is functionally dependent on a part (proper subset) of a candidate key
Third normal form (3NF) E.F. Codd (1971)[14]; see +also Carlo Zaniolo's equivalent but differently-expressed definition (1982)[15]  Every non-prime attribute is non-transitively dependent on every key of the table
Boyce-Codd normal form (BCNF) Raymond F. Boyce and E.F. Codd (1974)[16] Every non-trivial functional dependency in the table is a dependency on a superkey
Fourth normal form (4NF)   Ronald Fagin (1977)[17] Every non-trivial multivalued dependency in the table is a dependency on a superkey
Fifth normal form (5NF) Ronald Fagin (1979)[18] Every non-trivial join dependency in the table is implied by the superkeys of the table
Domain/key normal form (DKNF) Ronald Fagin (1981)[19] Every constraint on the table is a logical consequence of the table's domain constraints and key constraints
Sixth normal form (6NF) C.J. Date, Hugh Darwen, and Nikos Lorentzos (2002)[5] Table features no non-trivial join dependencies at all (with reference to generalized join operator)





--------




- wrong title (sorry)
- this is ETL storage, just to be for-warned.
- problem
-- our schema sucked
--- source => diffrent DB for 'same' info
---- cascade lookups
---- data gaps
- goal
-- single location
-- source every data point
=> over normalize everything
- pros!
-- reports, cake!
-- debugging data, cake!
-- specialized data use, cake!
- cons!
-- slow! to read from
--- SOLUTION: summary tables currently, possibly KV store down the road
--- HIDDEN PRO: abstraction for reads means you can bomb your data (2 day alter table, sure!)
-- complex structure => staff confusion
--- SOLUTION: abstract everything to code (hello ORM)
--- HIDDEN PRO: abstraction means you are free to change schema at will.

